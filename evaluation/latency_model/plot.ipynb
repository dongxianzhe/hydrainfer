{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-25T02:57:57.568732200Z",
     "start_time": "2025-08-25T02:57:57.546734500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = os.getcwd()\n",
    "tables_dir = os.path.join(base_dir, 'table')\n",
    "os.makedirs(tables_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$S$      The length of prompt\n",
      "$G$      The length of generated tokens\n",
      "$B$      The number of batched requests\n",
      "$T$      The number of tokens per image\n",
      "$L$      The number of model layers\n",
      "$H$      Input dimension of the hidden layer\n",
      "$M$      The number of attention heads\n",
      "$D$      The hidden stage per head\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table_headers = [\"Notation\", \"Description\"]\n",
    "table_data = [\n",
    "    (\"$S$\", \"The length of prompt\"),\n",
    "    (\"$G$\", \"The length of generated tokens\"),\n",
    "    (\"$B$\", \"The number of batched requests\"),\n",
    "    (\"$T$\", \"The number of tokens per image\"), \n",
    "    (\"$L$\", \"The number of model layers\"),\n",
    "    (\"$H$\", \"Input dimension of the hidden layer\"),\n",
    "    (\"$M$\", \"The number of attention heads\"),\n",
    "    (\"$D$\", \"The hidden stage per head\"\"\"),\n",
    "    # (\"$n$\", \"number of images\"), \n",
    "    # (\"$m$\", \"number of tokens\"), \n",
    "    # (\"$s$\", \"number of kv cache tokens of each request\"), \n",
    "    # (\"$h_v$, $h_l$\", \"hidden size of\"), \n",
    "    # (\"$d_v$, $d_l$\", \"head dimension of attention layer of vision mode\"), \n",
    "    # (\"$q_v$, $q_l$\", \"number of heads of attention layer of vision mode\"), \n",
    "    # (\"$i_v$\", \"intermediate size of feed forward layer of vision model\"), \n",
    "    # (\"$k$\", \"kernel size of the convolutional layer\"), \n",
    "    # (\"$a$\", \"width and height of image\"), \n",
    "    # (\"$c$\", \"number of channels of images\"), \n",
    "    # (\"$L_v$\", \"number of layers of vision model\"), \n",
    "    # (\"$h_l$\", \"hidden size of language model\"), \n",
    "    # (\"$d_l$\", \"head dimension of language model\"), \n",
    "    # (\"$q_l$\", \"number of query heads of attention layer of language model\"), \n",
    "    # (\"$k_l$\", \"number of key heads or value heads of attention layer of language model\"), \n",
    "    # (\"$i_l$\", \"intermediate size of feed forward layer of language model\"), \n",
    "    # (\"$V$\", \"vocabulary size of language model\"), \n",
    "]\n",
    "\n",
    "latex_table = \"\\\\begin{tabular}{ll}\\n\\\\toprule\\n\"\n",
    "headers_str = \"\"\n",
    "for i, head in enumerate(table_headers):\n",
    "    headers_str += head\n",
    "    if i != len(table_headers) - 1:\n",
    "        headers_str += \" & \"\n",
    "latex_table += f\"{headers_str} \\\\\\\\\\n\\\\midrule\\n\"\n",
    "for row in table_data:\n",
    "    latex_table += f\"{row[0]} & {row[1]} \\\\\\\\\\n\"\n",
    "latex_table += \"\\\\bottomrule\\n\\\\end{tabular}\\n\"\n",
    "\n",
    "with open(os.path.join(tables_dir, \"latency_model_notation.tex\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "for row in table_data:\n",
    "    print(f\"{row[0]:<8} {row[1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-25T02:57:57.595729600Z",
     "start_time": "2025-08-25T02:57:57.564731700Z"
    }
   },
   "id": "b48d40fbb72df89",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX table saved to operator_table.tex\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"operation\": \"QKVO Projection\",\n",
    "        \"rows\": [\n",
    "            {\"E/P/D\": \"encode\",  \"FLOPS\": r\"$8BTH^2$\", \"Memory\": r\"$8BTH+4H^2$\"},\n",
    "            {\"E/P/D\": \"prefill\", \"FLOPS\": r\"$8BSH^2$\", \"Memory\": r\"$8BSH+4H^2$\"},\n",
    "            {\"E/P/D\": \"decode\", \"FLOPS\": r\"$8BH$\", \"Memory\": r\"$8BH+4H^2$\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"FFN\",\n",
    "        \"rows\": [\n",
    "            {\"E/P/D\": \"encode\",  \"FLOPS\": r\"$16BTH^2$\", \"Memory\": r\"$4BTH+8H^2$\"},\n",
    "            {\"E/P/D\": \"prefill\", \"FLOPS\": r\"$16BSH^2$\", \"Memory\": r\"$4BSH+8H^2$\"},\n",
    "            {\"E/P/D\": \"decode\", \"FLOPS\": r\"$16BH^2$\", \"Memory\": r\"$4BH+8H^2$\"},\n",
    "        ]\n",
    "    }, \n",
    "    {\n",
    "        \"operation\": \"Attention\",\n",
    "        \"rows\": [\n",
    "            {\"E/P/D\": \"encode\", \"FLOPS\": r\"$4BT^2H$\", \"Memory\": r\"$4BTH+2BT^2M$\"},\n",
    "            {\"E/P/D\": \"prefill\", \"FLOPS\": r\"$4BS^2H$\", \"Memory\": r\"$4BSH+2BS^2M$\"},\n",
    "            {\"E/P/D\": \"decode\", \"FLOPS\": r\"$4BSH$\", \"Memory\": r\"$4BSM+2BH(S+1)$\"},\n",
    "        ]\n",
    "    }, \n",
    "]\n",
    "\n",
    "def generate_latex_table(data):\n",
    "    lines = []\n",
    "    lines.append(r\"\\begin{tabular}{lcccc}\")\n",
    "    lines.append(r\"\\toprule\")\n",
    "    lines.append(r\"Operation & E/P/D & FLOPS & Memory Access \\\\\")\n",
    "    lines.append(r\"\\midrule\")\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        op_name = item[\"operation\"]\n",
    "        rows = item[\"rows\"]\n",
    "        first_row = True\n",
    "        for row in rows:\n",
    "            if first_row:\n",
    "                line = r\"\\multirow{{{}}}{{*}}{{{}}} & {} & {} & {} \\\\\".format(\n",
    "                    len(rows), op_name, row[\"E/P/D\"], row[\"FLOPS\"], row[\"Memory\"]\n",
    "                )\n",
    "                first_row = False\n",
    "            else:\n",
    "                line = r\" & {} & {} & {} \\\\\".format(row[\"E/P/D\"], row[\"FLOPS\"], row[\"Memory\"])\n",
    "            lines.append(line)\n",
    "        if i != len(data) - 1:\n",
    "            lines.append('\\hline')\n",
    "\n",
    "    lines.append(r\"\\bottomrule\")\n",
    "    lines.append(r\"\\end{tabular}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "latex_code = generate_latex_table(data)\n",
    "with open(os.path.join(tables_dir, \"latency_model.tex\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_code)\n",
    "\n",
    "print(\"LaTeX table saved to operator_table.tex\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-25T02:57:57.648731300Z",
     "start_time": "2025-08-25T02:57:57.589731900Z"
    }
   },
   "id": "c7834c50513731fd",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87509f1207942b09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
